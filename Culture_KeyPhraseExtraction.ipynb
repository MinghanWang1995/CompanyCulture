{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 26: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14858)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17347)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8 (pandas\\_libs\\parsers.c:23041)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 26: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-df00b83d5e87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mculture_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'culture.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mculture_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1748\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1749\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas\\_libs\\parsers.c:10862)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas\\_libs\\parsers.c:11138)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas\\_libs\\parsers.c:12175)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas\\_libs\\parsers.c:14136)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14972)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17347)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8 (pandas\\_libs\\parsers.c:23041)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 26: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "culture_data = pd.read_csv('culture.csv')\n",
    "print (culture_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120447"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(culture_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = culture_data['industry']\n",
    "industry_name = pd.DataFrame(industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>Biotech &amp; Pharmaceuticals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>Oil, Gas, Energy &amp; Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>Government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>Business Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>Consumer Discretionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>Travel &amp; Tourism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10481</th>\n",
       "      <td>Telecommunications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11643</th>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15753</th>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16685</th>\n",
       "      <td>Transportation &amp; Logistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18984</th>\n",
       "      <td>Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24054</th>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058</th>\n",
       "      <td>Construction, Repair &amp; Maintenance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29287</th>\n",
       "      <td>Restaurants, Bars &amp; Food Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41467</th>\n",
       "      <td>Mining &amp; Metals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74326</th>\n",
       "      <td>Accounting &amp; Legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78381</th>\n",
       "      <td>Agriculture &amp; Forestry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91559</th>\n",
       "      <td>Financials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115703</th>\n",
       "      <td>Arts, Entertainment &amp; Recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115708</th>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  industry\n",
       "0                            Manufacturing\n",
       "946              Biotech & Pharmaceuticals\n",
       "1496                Information Technology\n",
       "2943                                Retail\n",
       "3104          Oil, Gas, Energy & Utilities\n",
       "3162                             Insurance\n",
       "3942                               Finance\n",
       "4071                            Government\n",
       "4096                     Business Services\n",
       "5310                Consumer Discretionary\n",
       "6046                      Travel & Tourism\n",
       "10481                   Telecommunications\n",
       "10500                          Health Care\n",
       "11643                  Aerospace & Defense\n",
       "15753                          Real Estate\n",
       "16685           Transportation & Logistics\n",
       "18984                                Media\n",
       "24054                               Energy\n",
       "24058   Construction, Repair & Maintenance\n",
       "29287    Restaurants, Bars & Food Services\n",
       "41467                      Mining & Metals\n",
       "74326                   Accounting & Legal\n",
       "78381               Agriculture & Forestry\n",
       "91559                           Financials\n",
       "115703    Arts, Entertainment & Recreation\n",
       "115708                           Utilities"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industry_name.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['gt','br','li','p','div']\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    #tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in text if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = list(filter(None, [pattern.sub('', token) for token in tokens]))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "Manufacturing = []\n",
    "for i in range(120447):\n",
    "    if culture_data.at[i,'industry'] == 'Utilities':\n",
    "        single_JD=culture_data.at[i,'job_desc']\n",
    "        Manufacturing.append(single_JD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Manufacturing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_corpus = []    \n",
    "for text in Manufacturing:\n",
    "    text = tokenize_text(text.lower())\n",
    "    token_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokenize=[]\n",
    "for text in token_corpus:\n",
    "    per_clean_tokenize = remove_characters_after_tokenization(text)\n",
    "    clean_tokenize.append(per_clean_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords_corpus = []    \n",
    "for text in clean_tokenize:\n",
    "    text = remove_stopwords(text)\n",
    "    remove_stopwords_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_text_corpus = []    \n",
    "for text in remove_stopwords_corpus:\n",
    "    text = keep_text_characters(text)\n",
    "    keep_text_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Manufacturing = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Manufacturing = str.join(norm_Manufacturing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Biotech_Pharmaceuticals = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Biotech_Pharmaceuticals = str.join(norm_Biotech_Pharmaceuticals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Information_Technology = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Information_Technology = str.join(norm_Information_Technology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_retail = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "retail = str.join(norm_retail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Oil_Gas_Energy_Utilities = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Oil_Gas_Energy_Utilities = str.join(norm_Oil_Gas_Energy_Utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Insurance = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Insurance = str.join(norm_Insurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Finance = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Finance = str.join(norm_Finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Government = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Government = str.join(norm_Government)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Business_Services = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Business_Services = str.join(norm_Business_Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Consumer_Discretionary = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Consumer_Discretionary = str.join(norm_Consumer_Discretionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Travel_Tourism = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Travel_Tourism = str.join(norm_Travel_Tourism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Telecommunications = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Telecommunications = str.join(norm_Telecommunications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Health_Care = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Health_Care = str.join(norm_Health_Care)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Aerospace_Defense = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Aerospace_Defense = str.join(norm_Aerospace_Defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Real_Estate = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Real_Estate = str.join(norm_Real_Estate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Transportation_Logistics = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Transportation_Logistics = str.join(norm_Transportation_Logistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Media = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Media = str.join(norm_Media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Energy = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Energy = str.join(norm_Energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Construction_Repair_Maintenance = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Construction_Repair_Maintenance = str.join(norm_Construction_Repair_Maintenance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Restaurants_Bars_FoodServices = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Restaurants_Bars_FoodServices = str.join(norm_Restaurants_Bars_FoodServices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Mining_Metals = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Mining_Metals = str.join(norm_Mining_Metals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Accounting_Legal = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Accounting_Legal = str.join(norm_Accounting_Legal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Agriculture_Forestry = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Agriculture_Forestry = str.join(norm_Agriculture_Forestry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Financials = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Financials = str.join(norm_Financials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Arts_Entertainment_Recreation = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Arts_Entertainment_Recreation = str.join(norm_Arts_Entertainment_Recreation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Utilities = keep_text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = ' '\n",
    "Utilities = str.join(norm_Utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda2\\envs\\py3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "from gensim import corpora, models\n",
    "import itertools\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_phrase(corpus):\n",
    "    key_phrase = []\n",
    "    for text in corpus:\n",
    "        finder = TrigramCollocationFinder.from_documents([tokenize_text(text)])\n",
    "        trigram_measures = TrigramAssocMeasures()                                                \n",
    "        aaa = []\n",
    "        for text in finder.nbest(trigram_measures.raw_freq, 3):\n",
    "            peraaa = ' '.join(text)\n",
    "            aaa.append(peraaa)\n",
    "        key_phrase.append(aaa)\n",
    "    return key_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tfidf_ngrams(corpus, ngram_val=1, limit=5):\n",
    "\n",
    "    valid_keyphrase = get_key_phrase(corpus)\n",
    "    \n",
    "    dictionary = corpora.Dictionary(valid_keyphrase)\n",
    "    corpus = [dictionary.doc2bow(text) for text in valid_keyphrase]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "    weighted_phrases = {dictionary.get(id): round(value,3) \n",
    "                        for doc in corpus_tfidf \n",
    "                        for id, value in doc}\n",
    "                            \n",
    "    weighted_phrases = sorted(weighted_phrases.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    return weighted_phrases[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accurate timely cost', 0.57699999999999996),\n",
       " ('achieve project objectives', 0.57699999999999996),\n",
       " ('area project controls', 0.57699999999999996),\n",
       " ('ability become isa', 0.57699999999999996),\n",
       " ('access certain critical', 0.57699999999999996),\n",
       " ('requirements applicant must', 0.57699999999999996),\n",
       " ('account applications set', 0.57699999999999996),\n",
       " ('account information collection', 0.57699999999999996),\n",
       " ('account information insure', 0.57699999999999996),\n",
       " ('accurate plant records', 0.57699999999999996),\n",
       " ('area network lan', 0.57699999999999996),\n",
       " ('assigned posted startwire', 0.57699999999999996),\n",
       " ('air quality control', 0.57699999999999996),\n",
       " ('electrical controls engineer', 0.57699999999999996),\n",
       " ('engineer provide engineering', 0.57699999999999996),\n",
       " ('45m line device', 0.57699999999999996),\n",
       " ('ability work diverse', 0.57699999999999996),\n",
       " ('allocation functions responsible', 0.57699999999999996),\n",
       " ('abnormal conditions return', 0.57699999999999996),\n",
       " ('accountable effective implementation', 0.57699999999999996),\n",
       " ('achieve corporate strategy', 0.57699999999999996),\n",
       " ('analyst substation office', 0.57699999999999996),\n",
       " ('analyst support substation', 0.57699999999999996),\n",
       " ('substation engineering operations', 0.57699999999999996)]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_tfidf_ngrams(keep_text_corpus, ngram_val=3, limit=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency',\n",
    "                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                     ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = [Manufacturing,Biotech_Pharmaceuticals,Information_Technology,retail,Oil_Gas_Energy_Utilities,Insurance,Finance,Government,Business_Services,Consumer_Discretionary,Travel_Tourism,Telecommunications,Health_Care,Aerospace_Defense,Real_Estate,Transportation_Logistics,Media,Energy,Construction_Repair_Maintenance,Restaurants_Bars_FoodServices,Mining_Metals,Accounting_Legal,Agriculture_Forestry,Financials,Arts_Entertainment_Recreation,Utilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus_without_Manufacturing = [Biotech_Pharmaceuticals,Information_Technology,retail,Oil_Gas_Energy_Utilities,Insurance,Finance,Government,Business_Services,Consumer_Discretionary,Travel_Tourism,Telecommunications,Health_Care,Aerospace_Defense,Real_Estate,Transportation_Logistics,Media,Energy,Construction_Repair_Maintenance,Restaurants_Bars_FoodServices,Mining_Metals,Accounting_Legal,Agriculture_Forestry,Financials,Arts_Entertainment_Recreation,Utilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_corpus_without_Manufacturing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "Manufacturing = [Manufacturing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpus_without_Manufacturing,\n",
    "                                                        feature_type='tfidf',\n",
    "                                                        ngram_range=(3, 3), \n",
    "                                                        min_df=0.4, max_df=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs_tfidf = tfidf_vectorizer.transform(Manufacturing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(doc_features, corpus_features,\n",
    "                              top_n=3):\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Analysis using Cosine Similarity\n",
      "============================================================\n",
      "Top 25 similar docs:\n",
      "----------------------------------------\n",
      "Doc num: 1 Similarity Score: 0.679\n",
      "----------------------------------------\n",
      "Doc num: 5 Similarity Score: 0.544\n",
      "----------------------------------------\n",
      "Doc num: 2 Similarity Score: 0.513\n",
      "----------------------------------------\n",
      "Doc num: 17 Similarity Score: 0.51\n",
      "----------------------------------------\n",
      "Doc num: 8 Similarity Score: 0.507\n",
      "----------------------------------------\n",
      "Doc num: 6 Similarity Score: 0.475\n",
      "----------------------------------------\n",
      "Doc num: 13 Similarity Score: 0.452\n",
      "----------------------------------------\n",
      "Doc num: 4 Similarity Score: 0.412\n",
      "----------------------------------------\n",
      "Doc num: 21 Similarity Score: 0.356\n",
      "----------------------------------------\n",
      "Doc num: 16 Similarity Score: 0.305\n",
      "----------------------------------------\n",
      "Doc num: 20 Similarity Score: 0.288\n",
      "----------------------------------------\n",
      "Doc num: 3 Similarity Score: 0.286\n",
      "----------------------------------------\n",
      "Doc num: 12 Similarity Score: 0.273\n",
      "----------------------------------------\n",
      "Doc num: 15 Similarity Score: 0.249\n",
      "----------------------------------------\n",
      "Doc num: 9 Similarity Score: 0.187\n",
      "----------------------------------------\n",
      "Doc num: 14 Similarity Score: 0.136\n",
      "----------------------------------------\n",
      "Doc num: 11 Similarity Score: 0.133\n",
      "----------------------------------------\n",
      "Doc num: 18 Similarity Score: 0.114\n",
      "----------------------------------------\n",
      "Doc num: 10 Similarity Score: 0.102\n",
      "----------------------------------------\n",
      "Doc num: 22 Similarity Score: 0.07\n",
      "----------------------------------------\n",
      "Doc num: 24 Similarity Score: 0.027\n",
      "----------------------------------------\n",
      "Doc num: 7 Similarity Score: 0.022\n",
      "----------------------------------------\n",
      "Doc num: 19 Similarity Score: 0.022\n",
      "----------------------------------------\n",
      "Doc num: 23 Similarity Score: 0.018\n",
      "----------------------------------------\n",
      "Doc num: 25 Similarity Score: 0.006\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('Document Similarity Analysis using Cosine Similarity')\n",
    "print ('='*60)\n",
    "for index, doc in enumerate(Manufacturing):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                             tfidf_features,\n",
    "                                             top_n=25)\n",
    "    #print ('Document',index+1 ,':', doc)\n",
    "    print ('Top', len(top_similar_docs), 'similar docs:')\n",
    "    print ('-'*40)\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        print ('Doc num: {} Similarity Score: {}'.format(doc_index+1,\n",
    "                                                                 sim_score)) \n",
    "        print ('-'*40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
